# OCR assignment report

## Feature Extraction (Max 200 Words)
[Describe and justify the design of your feature extraction. Note,
feature extraction includes all the steps that you have taken to
produce the 10 dimensional feature vector starting from the initial
image of the page.]
I have chosen principal component analysis to produce a 10 dimensional
feature vector. Principal component analysis removes correlated features
and these features don't make any difference to the decision making in the
classifier. It essentially rotating the axis so that it maximises the
spread of the data. I have added an extra function called learn_pca which
generates the matrix of eigenvectors of the covariance matrix from th training
data. I then convert the numpy array to a list since numpy arrays aren't json
serializable for it to then be added to the model dictionary. I can then
use the matrix to reduce the dimensions on my train and test data by taking the
dot product between the difference between the mean and each feature vector
and matrix generated by pca.

## Classifier (Max 200 Words)
[Describe and justify the design of your classifier and any
associated classifier training stage.]
Firstly, I implemented a nearest neighbour classifier. I opted for cosine
similarity to calculate the distance between points. This reduces the effect
of brightness. From all the distances I am able to get the indices of the
nearest neighbour and select those from the labels. I then attempted to
to improve he algorithm by doing a k nearest neighbour classifier. Similar
to my nearest neighbour classifier I use cosine similarity to calculate
distances. Unlike simple nearest neighbour I sort the the distances in
order of size and take the first k. Then I assign the label to the one
that is most frequent by taking the mode. by trying different values
of k, the highest average score seemed to peak a around k=9. I assume
this is because higher values of k reduce the effect of noise on the 
classification.

## Error Correction (Max 200 Words)
[Describe and justify the design of any post classification error
correction that you may have attempted.]
I was able to form words from the labels by taking the difference between
the horizontal distances of two bounding boxes. Previously, I was calculating
the centre point of a box and then taking the difference, however this didnt
didn't end up working, probably because of punctuation having different
bounding box sizes. I ended up using two dictionaries of words. One simply
with a huge list of existing words and one with over 40000 words in
order of frequency. The reason I used both in the end was because that
the second list contained words like 'aaaaa' and 'rrrr'. I had to
add both dictionaries to the model during the training stage since we're not
allowed to submit text files separately. For all the words that weren't 
existing words, I make edits on the word by replacing letters with with 
every letter in the alphabet. For every single edit on the word I check 
whether the word exists and use the word which is has the highest probability 
of being used. From the corrections I replaced labels with the corrected label.

## Performance
The percentage errors (to 1 decimal place) for the development data are
as follows:
- Page 1: 92.9%
- Page 2: 95.7%
- Page 3: 93.4%
- Page 4: 81.3%
- Page 5: 63.4%
- Page 6: 55.8%
- Average Score: 80.4%

## Other information (Optional, Max 100 words)
[Optional: highlight any significant aspects of your system that are
NOT covered in the sections above]
I used SciPys gaussian filter on the each of the letter images in the 
training stage. This improved the results I was getting from my classifier
as it removed unwanted noise. When using the gaussian filter you have to 
give a value for the standard deviation and seemed to work best when 
sigma=0.9
